#
# Example of configuration for crawling all pages of the web sites in the seeds
#

# Configure ELASTICSEARCH and FILES data formats
target_storage.data_formats:
  - ELASTICSEARCH
  - WARC

target_storage.data_format.elasticsearch.rest.hosts:
  - http://elasticsearch:9200

# Enables "scope" to only crawl pages that belong to domains of seed URLs
link_storage.link_strategy.use_scope: false

# Perform breadth-search crawl
link_storage.link_classifier.type: MaxDepthLinkClassifier
link_storage.link_classifier.max_depth: 1

# Select URLs from all domains during link selection phase,
link_storage.link_selector: MaximizeWebsitesLinkSelector

# Configure the minimum time interval (in milliseconds) to wait between requests
# to the same host to avoid overloading servers. If you are crawling your own
# web site, you can descrease this value to speed-up the crawl.
link_storage.scheduler.host_min_access_interval: 4000

# Enables discovery of links using the Sitemaps protocol
link_storage.download_sitemap_xml: false

# Configure the User-Agent of the crawler
crawler_manager.downloader.user_agent.name: ACHE
crawler_manager.downloader.user_agent.url: https://github.com/ViDA-NYU/ache

# Limit on number of crawled webpages per domain
link_storage.max_pages_per_domain: 1

# Save irrelevant pages
target_storage.store_negative_pages: true